Pour indexer de nouvelles ressources, un robot procède en suivant récursivement les hyperliens trouvés à partir d'une page pivot. Par la suite, il est avantageux de mémoriser l'URL de chaque ressource récupérée et d'adapter la fréquence des visites à la fréquence observée de mise à jour de la ressource. Toutefois, si le robot respecte les règles du fichier robots.txt, alors de nombreuses ressources échappent à cette exploration récursive. Cet ensemble de ressources inexploré est appelé Web profond ou Web invisible.
Un fichier d'exclusion (robots.txt) placé dans la racine d'un site Web permet de donner aux robots une liste de ressources à ignorer. Cette convention permet de réduire la charge du serveur Web et d'éviter des ressources sans intérêt. Toutefois, certains robots ne se préoccupent pas de ce fichier.
Deux caractéristiques du Web compliquent le travail du robot d'indexation : le volume de données et la bande passante. Les capacités de traitement et de stockage des ordinateurs ainsi que le nombre d'internautes ayant fortement progressé, cela lié au développement d'outils de maintenance de pages de type Web 2.0 permettant à n'importe qui de mettre facilement en ligne des contenus, le nombre et la complexité des pages et objets multimédia disponibles, et leur modification, s'est considérablement accru dans la première décennie du xxie siècle. Le débit autorisé par la bande passante n'ayant pas connu une progression équivalente, le problème est de traiter un volume toujours croissant d'information avec un débit relativement limité. Les robots ont donc besoin de donner des priorités à leurs téléchargements.
Le comportement d'un robot d'indexation résulte de la combinaison des principes suivants :
Un principe de sélection, qui définit quelles pages télécharger ;
Un principe de re-visite, qui définit quand vérifier s'il y a des changements dans les pages ;
Un principe de politesse, qui définit comment éviter les surcharges de pages Web (délais en général) ;
Un principe de parallélisation, qui définit comment coordonner les robots d'indexations distribués.
Le serveur HTTP le plus utilisé est Apache HTTP Server qui sert environ 55 % des sites web en janvier 2013 selon Netcraft6.
Le serveur HTTP le plus utilisé dans les 1 000 sites les plus actifs est en revanche Nginx avec 38,2 % de parts de marché en 2016 selon w3techs et 53,9 % en avril 20174
Historiquement, d'autres serveurs HTTP importants furent CERN httpd, développé par les inventeurs du Web, abandonné le 15 juillet 1996 et NCSA HTTPd, développé au NCSA en même temps que NCSA Mosaic, abandonné mi-1994, ainsi que WebObjects.
Il existe aussi des serveurs HTTP qui sont des serveurs d'applications capables de faire serveur HTTP, comme Caudium et GlassFish. À l'inverse, on peut trouver des serveurs HTTP spécialisés dans un service distinct comme : HTTP File Server qui est uniquement destiné au partage de fichiers Le logiciel serveur HTTP ou daemon HTTP est le logiciel prenant en charge les requêtes client-serveur du protocole HTTP développé pour le World Wide Web. Ces logiciels intègrent généralement des modules permettant d'exécuter un langage serveur comme PHP pour générer des pages web dynamiques. Les plus connus sont Apache, Nginx, IIS, et Lighttpd.